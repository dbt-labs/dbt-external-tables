
version: 2.1

orbs:
  azure-cli: circleci/azure-cli@1.1.0

jobs:
  
  integration-redshift:
    docker:
      - image: cimg/python:3.9.9
    steps:
      - checkout
      - run:
          name: "Run Tests - Redshift"
          command: ./run_test.sh redshift
      - store_artifacts:
          path: ./logs
      
  integration-snowflake:
    docker:
      - image: cimg/python:3.9.9
    steps:
      - checkout
      - run:
          name: "Run Tests - Snowflake"
          command: ./run_test.sh snowflake
      - store_artifacts:
          path: ./logs
          
  integration-bigquery:
    environment:
      BIGQUERY_SERVICE_KEY_PATH: "/home/circleci/bigquery-service-key.json"
    docker:
      - image: cimg/python:3.9.9
    steps:
      - checkout
      - run:
          name: "Set up credentials"
          command: echo $BIGQUERY_SERVICE_ACCOUNT_JSON > ${HOME}/bigquery-service-key.json
      - run:
          name: "Run Tests - BigQuery"
          command: ./run_test.sh bigquery
      - store_artifacts:
          path: ./logs

  integration-databricks:
    environment:
      ODBC_DRIVER: Simba
    docker:
      # image based on `fishtownanalytics/test-container` w/ Simba ODBC Spark driver installed
      - image: 828731156495.dkr.ecr.us-east-1.amazonaws.com/dbt-spark-odbc-test-container:latest
        aws_auth:
          aws_access_key_id: $AWS_ACCESS_KEY_ID_STAGING
          aws_secret_access_key: $AWS_SECRET_ACCESS_KEY_STAGING
    steps:
      - checkout
      - run:
          name: "Run Tests - Databricks"
          command: ./run_test.sh databricks
      - store_artifacts:
          path: ./logs

  # DISABLED FOR NOW
  integration-synapse:
    docker:
      - image: dataders/pyodbc:1.2
    steps:
      - checkout
      - run: &gnupg2
          name: az cli dep
          command: apt-get install gnupg2 -y
      - azure-cli/install
      - azure-cli/login-with-service-principal: &azure-creds
          azure-sp: DBT_AZURE_SP_NAME
          azure-sp-password: DBT_AZURE_SP_SECRET
          azure-sp-tenant: DBT_AZURE_TENANT
      - run:
          name: resume Synapse pool/db
          command: az synapse sql pool resume --name $DBT_SYNAPSE_DB --workspace-name $DBT_SYNAPSE_SERVER --resource-group dbt-msft
      - run:
          name: "Run Tests - synapse"
          command: ./run_test.sh synapse
      - run:
          name: pause Synapse pool/db
          command: az synapse sql pool resume --name $DBT_SYNAPSE_DB --workspace-name $DBT_SYNAPSE_SERVER --resource-group dbt-msft
      - store_artifacts:
          path: ./logs

  # DISABLED FOR NOW
  integration-azuresql:
    docker:
      - image: dataders/pyodbc:1.2
    steps:
      - checkout
      - run: *gnupg2
      - azure-cli/install
      - azure-cli/login-with-service-principal: *azure-creds
      - run:
          name: "Run Tests - azuresql"
          command: ./run_test.sh azuresql
      - store_artifacts:
          path: ./logs


workflows:
  version: 2
  test-all:
    jobs:
      - integration-redshift
      - integration-snowflake
      - integration-bigquery
      - integration-databricks
      #- integration-synapse
      #- integration-azuresql:
      #    requires:
      #      - integration-synapse
